# summer-iitb-pb-24
Research Internship in IIT Bombay under P Balamurugan - 2024 - Optimization Methods for Machine Learning Problems


This repository contains the work from my research internship at the IEOR Department, IIT Bombay. The project focused on implementing and analyzing different optimization methods used in training machine learning models.

The main focus was on Accelerated Alternating Minimization (AAM), an algorithm that integrates Alternating Minimization with Nesterov’s Momentum. I analyzed its theoretical convergence properties, studied the impact of momentum on alternating minimization, and conducted empirical evaluations against standard optimizers. The work examined AAM in both convex and non-convex settings, demonstrating its potential for faster and more stable convergence.

Work Summary

• Explored optimization algorithms for regression and classification problems.

•Implemented a wide range of optimizers and compared their convergence behavior.

• Studied the effects of learning rate, gradient noise, and problem type (convex vs non-convex) on training.

• Applied the methods to the Iris dataset and synthetic regression tasks.

Implemented Algorithms

• Gradient Descent (GD)

• Momentum-based Gradient Descent

• Nesterov Accelerated Gradient (NAG)

• Stochastic Gradient Descent (SGD)

• Adagrad

• RMSProp

• Adam

• Accelerated Alternating Minimization
